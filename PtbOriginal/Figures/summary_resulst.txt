TEST_NAME                                    LAST_TRAINING_PPL   LAST_VALID_PPL      TEST_PPL            AVERAGE_SPEED [WPS] STD_SPEED           
hidden_size = 128                            137.152595302       129.886872476       125.064161404       11173.8044194       97.5990856061       
hidden_size = 128                            130.685758897       125.043949716       120.648646119       11027.2286113       128.34885334        
hidden_size = 128                            124.83706474        120.872120462       116.282201441       11281.3936418       163.226281324       
hidden_size = 256                            96.5572282975       106.188510878       102.870341326       8874.81561386       69.3584135338       
hidden_size = 256                            90.5043785258       101.92401382        98.3112030671       8584.06923252       59.5255891612       
hidden_size = 256                            85.3553742247       98.3591601988       94.8025733374       8698.94656241       74.4517631619       
hidden_size = 512                            66.7117271938       94.9332687069       92.1504161602       5268.90355785       27.1248713353       
hidden_size = 512                            60.8135881311       91.5243387306       87.9890080728       5461.23670379       31.148582026        
hidden_size = 512                            56.8551065369       89.650105906        86.0613724782       5311.41491987       34.7783145477       
init_scale = 0.005, hidden_size = 128        125.430070014       122.018113701       116.753549328       11288.7535285       204.738568443       
init_scale = 0.005, hidden_size = 512        58.1604360958       90.2953329784       86.3674946855       5298.50329543       35.8753546262       
init_scale = 0.05, hidden_size = 128         124.602483407       120.465294438       115.813703248       11433.1415082       183.523624012       
init_scale = 0.05, hidden_size = 512         56.9165282049       89.3015302337       85.8495035936       5305.48983014       28.0358939148       
init_scale = 0.5, hidden_size = 128          155.305260436       139.75188496        133.847617021       11216.006993        184.094650348       
init_scale = 0.5, hidden_size = 512          139.211379722       130.720621157       126.068001429       5318.15859618       31.7814583816       
learning_rate = 0.1, lr_decay = 0.2          214.473120785       209.364821172       204.030530993       5316.43813196       40.2063687305       
learning_rate = 0.5, lr_decay = 0.2          90.101842649        106.377801752       103.563187375       5311.62581152       82.8411377281       
learning_rate = 1.0, lr_decay = 0.2          78.164309653        96.0926278438       92.7933277511       5324.21646869       27.2285806462       
learning_rate = 0.1, lr_decay = 0.8          160.068775953       163.691214126       159.673539932       5324.88927394       38.5772046305       
learning_rate = 0.5, lr_decay = 0.8          66.5606180633       96.0880604117       93.0609055945       5308.76861042       31.4432339535       
learning_rate = 1.0, lr_decay = 0.8          56.8052794411       88.9418014919       85.3885747575       5316.82312122       32.2298533837       
learning_rate = 0.1, lr_decay = 1.0          84.6243616305       118.666220792       117.514961435       5319.18410725       29.2820812814       
learning_rate = 0.5, lr_decay = 1.0          60.9703766418       97.5768567493       95.662554672        5317.94250759       38.0476889464       
learning_rate = 1.0, lr_decay = 1.0          64.2169959145       93.3527146564       91.0133975603       5313.88508758       35.454354454        
num_layers = 1                               52.6573685147       87.7519107465       84.3744260722       8781.36532219       50.3419870623       
num_layers = 2                               56.4768339075       88.8829618968       85.7698150926       5318.53566848       33.7947412835       
num_layers = 3                               61.5083958147       93.336077795        90.2381634589       3842.15623499       16.8316836539       
num_steps = 15                               59.9904117358       89.7558248994       86.1789922496       4796.69224811       29.0333439209       
num_steps = 35                               56.3721296939       88.4145286336       85.7701468334       5317.41036247       31.0033397971       
num_steps = 55                               58.1670236069       91.1253871699       87.9058994697       5522.29755491       23.3385644072       
num_steps = 75                               60.401354391        92.5607157658       89.4328456768       5606.59382486       23.2139719149       
num_steps = 20, batch_size = 10              63.8182516028       91.3286326749       87.3870721951       2841.4513943        14.6508965339       
num_steps = 35, batch_size = 10              57.4924750032       87.9364452456       84.7437115389       2967.67515076       15.22087806         
num_steps = 50, batch_size = 10              55.6267350947       87.5726027725       84.1103699672       3032.48302826       14.0742133653       
num_steps = 20, batch_size = 20              57.7875916347       89.4242984996       85.4032436959       5283.16152736       20.3111191451       
num_steps = 35, batch_size = 20              56.8357029426       89.0864894363       86.1965240453       5327.44717871       36.4638161674       
num_steps = 50, batch_size = 20              57.4257133438       89.8375525414       86.9126677214       5340.51699985       115.49695023        
num_steps = 20, batch_size = 40              58.0128054431       91.2084994116       87.4406133061       7340.33993587       59.7145691314       
num_steps = 35, batch_size = 40              59.9359234101       92.4088100743       89.0932161833       7222.8049915        37.5839067891       
num_steps = 50, batch_size = 40              63.9817409022       95.3052810117       91.6210699752       7685.0879011        27.1091634742       
loss_function = full_softmax                 56.7542644952       88.6126143302       85.5886927326       5318.92631504       33.449587994        
loss_function = sampled_softmax              4.73738702114       99.3000492983       94.9911110127       6250.81333876       41.0337241581       
loss_function = noise_contrastive_estimation 20.9865685814       141.537986256       136.036578184       6247.53362527       51.1300136048       
max_grad_norm = 1.0                          103.949339564       120.800061766       118.167436278       5322.48640774       34.147696656        
max_grad_norm = 5.0                          56.8390735772       89.2325368388       85.9045558497       5309.69567963       32.859168907        
max_grad_norm = 10.0                         61.9130428321       90.007088296        86.522705471        5328.37977596       34.4647697281       
optimizer = GradDesc                         56.5490842072       89.0828364608       85.6272921008       5324.85810857       34.476835595        
optimizer = Adadelta                         764.393259837       720.474468766       689.861311711       5188.60860303       30.8100446256       
optimizer = Adagrad                          67.8530221093       97.5118770709       93.7309132102       5261.33100152       32.3695712665       
optimizer = Adam                             66.7380748877       118.167332213       108.44684167        4806.05367467       27.1044767582       
optimizer = Momentum                         58.1349151652       88.06745513         84.2936646652       5264.82140434       31.0443838407       
