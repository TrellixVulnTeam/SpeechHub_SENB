TEST_NAME                                    LAST_TRAINING_PPL   LAST_VALID_PPL      TEST_PPL            AVERAGE_SPEED [WPS] STD_SPEED           
hidden_size = 128, embedded_size = 64        138.381577324       131.192459881       126.644126248       14426.0205323       548.91027612        
hidden_size = 128, embedded_size = 128       129.576879489       124.473365795       119.457460129       14125.3746587       405.155396869       
hidden_size = 128, embedded_size = 256       125.234809517       121.215957053       116.344332862       13360.7356146       343.850050451       
hidden_size = 256, embedded_size = 64        97.0861114056       107.028496099       103.505646806       10191.5877848       191.637390738       
hidden_size = 256, embedded_size = 128       90.2895086855       102.44212111        98.1966292483       10210.3013555       198.584426654       
hidden_size = 256, embedded_size = 256       85.3927294657       98.7509003458       94.9278889942       9860.09822881       180.499683321       
hidden_size = 512, embedded_size = 64        66.747672968        95.8412927224       92.2668566834       6292.71972665       41.7829354911       
hidden_size = 512, embedded_size = 128       60.7195650829       91.2410783164       88.4553440192       6540.51558882       76.8910085048       
hidden_size = 512, embedded_size = 256       56.7896746779       88.6787332448       85.8808478952       6281.8774392        44.0384955803       
init_scale = 0.05, learning_rate = 0.5       66.6704271513       96.9194181744       94.0757309488       6335.64389367       57.8092776983       
init_scale = 0.05, learning_rate = 1.0       56.4119364542       89.069514135        85.7990237417       6258.11500126       45.9284596429       
init_scale = 0.5, learning_rate = 0.5        177.566081206       156.366230334       150.792761656       6258.78875743       298.078640873       
init_scale = 0.5, learning_rate = 1.0        142.981659009       133.173026834       127.677332954       6342.32240394       59.2129046545       
init_scale = 1.0, learning_rate = 0.5        3.72684480336e+12   1657928141.71       1645730651.71       6253.25474974       72.7536560666       
init_scale = 1.0, learning_rate = 1.0        3.27094611094e+12   1370464236.12       1317819890.21       6281.71572892       91.7254212726       
num_steps = 35, batch_size = 10              165.256304241       153.227732212       147.229283124       3445.34059814       420.900738781       
num_steps = 50, batch_size = 10              155.357113315       144.021835795       137.652534192       3607.61234577       62.2335332696       
num_steps = 20, batch_size = 20              139.048810645       131.706312622       126.503164775       6207.58368206       52.3977715886       
num_steps = 35, batch_size = 20              146.044015045       135.16209852        130.418617055       6272.20443958       40.865241626        
num_steps = 50, batch_size = 20              156.041576357       141.761335239       136.851973488       6310.91027166       67.190413945        
num_steps = 20, batch_size = 40              166.859665352       149.672998549       143.499864169       8292.05352871       67.3907618391       
num_steps = 35, batch_size = 40              172.795275637       153.3642694         147.50252304        8195.55546129       156.388865025       
num_steps = 50, batch_size = 40              184.921568788       162.359636197       156.25076762        8549.61519563       94.6293110876       
loss_function = sequence_loss_by_example     56.6867396891       89.5831306917       86.2078655298       6265.59047104       46.6597685975       
loss_function = sampled_softmax              4.73432494077       99.2882594862       94.6205924279       7726.34937888       106.450783159       
num_layers = 1                               52.6921702567       87.5681781634       84.5104527624       9940.31622856       95.1318983981       
num_layers = 2                               56.4965749078       88.8383999882       85.771145579        6342.03784454       41.1493100153       
num_layers = 3                               61.3721783411       93.1343513798       90.0432668028       4555.96327751       46.5209465532       
optimizer = GradDesc                         56.9022104806       89.4754055443       86.0997150091       6275.36989142       41.3759592023       
optimizer = Adadelta                         764.641847259       720.282892057       689.946770171       6045.15815032       62.5917770119       
optimizer = Adagrad                          67.7529904352       97.1930519635       94.1228444208       6175.04669145       70.5423117638       
optimizer = Adam                             47.9043449645       134.611658208       118.536048163       5735.24362097       60.2549901788       
optimizer = Momentum                         57.9869919516       87.9145579491       84.5414474045       6118.60344509       88.2577326558       
