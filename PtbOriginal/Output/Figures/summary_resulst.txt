TEST_NAME                                    LAST_TRAINING_PPL   LAST_VALID_PPL      TEST_PPL            AVERAGE_SPEED [WPS] STD_SPEED           
hidden_size = 128                             & 137.1526   & 129.8869   & 125.0642   & 11173.8044 \\ \hline
hidden_size = 128                             & 130.6858   & 125.0439   & 120.6486   & 11027.2286 \\ \hline
hidden_size = 128                             & 124.8371   & 120.8721   & 116.2822   & 11281.3936 \\ \hline
hidden_size = 256                             & 96.5572    & 106.1885   & 102.8703   & 8874.8156  \\ \hline
hidden_size = 256                             & 90.5044    & 101.9240   & 98.3112    & 8584.0692  \\ \hline
hidden_size = 256                             & 85.3554    & 98.3592    & 94.8026    & 8698.9466  \\ \hline
hidden_size = 512                             & 66.7117    & 94.9333    & 92.1504    & 5268.9036  \\ \hline
hidden_size = 512                             & 60.8136    & 91.5243    & 87.9890    & 5461.2367  \\ \hline
hidden_size = 512                             & 56.8551    & 89.6501    & 86.0614    & 5311.4149  \\ \hline
0                                             & 137.1526   & 129.8869   & 125.0642   & 11173.8044 \\ \hline
init_scale = 0.005, hidden_size = 128         & 125.4301   & 122.0181   & 116.7535   & 11288.7535 \\ \hline
init_scale = 0.005, hidden_size = 512         & 58.1604    & 90.2953    & 86.3675    & 5298.5033  \\ \hline
init_scale = 0.05, hidden_size = 128          & 124.6025   & 120.4653   & 115.8137   & 11433.1415 \\ \hline
init_scale = 0.05, hidden_size = 512          & 56.9165    & 89.3015    & 85.8495    & 5305.4898  \\ \hline
init_scale = 0.5, hidden_size = 128           & 155.3053   & 139.7519   & 133.8476   & 11216.0070 \\ \hline
init_scale = 0.5, hidden_size = 512           & 139.2114   & 130.7206   & 126.0680   & 5318.1586  \\ \hline
learning_rate = 0.1, lr_decay = 0.2           & 214.4731   & 209.3648   & 204.0305   & 5316.4381  \\ \hline
learning_rate = 0.5, lr_decay = 0.2           & 90.1018    & 106.3778   & 103.5632   & 5311.6258  \\ \hline
learning_rate = 1.0, lr_decay = 0.2           & 78.1643    & 96.0926    & 92.7933    & 5324.2165  \\ \hline
learning_rate = 0.1, lr_decay = 0.8           & 160.0688   & 163.6912   & 159.6735   & 5324.8893  \\ \hline
learning_rate = 0.5, lr_decay = 0.8           & 66.5606    & 96.0881    & 93.0609    & 5308.7686  \\ \hline
learning_rate = 1.0, lr_decay = 0.8           & 56.8053    & 88.9418    & 85.3886    & 5316.8231  \\ \hline
learning_rate = 0.1, lr_decay = 1.0           & 84.6244    & 118.6662   & 117.5150   & 5319.1841  \\ \hline
learning_rate = 0.5, lr_decay = 1.0           & 60.9704    & 97.5769    & 95.6626    & 5317.9425  \\ \hline
learning_rate = 1.0, lr_decay = 1.0           & 64.2170    & 93.3527    & 91.0134    & 5313.8851  \\ \hline
num_layers = 1                                & 52.6574    & 87.7519    & 84.3744    & 8781.3653  \\ \hline
num_layers = 2                                & 56.4768    & 88.8830    & 85.7698    & 5318.5357  \\ \hline
num_layers = 3                                & 61.5084    & 93.3361    & 90.2382    & 3842.1562  \\ \hline
num_steps = 15                                & 59.9904    & 89.7558    & 86.1790    & 4796.6922  \\ \hline
num_steps = 35                                & 56.3721    & 88.4145    & 85.7701    & 5317.4104  \\ \hline
num_steps = 55                                & 58.1670    & 91.1254    & 87.9059    & 5522.2976  \\ \hline
num_steps = 75                                & 60.4014    & 92.5607    & 89.4328    & 5606.5938  \\ \hline
num_steps = 20, batch_size = 10               & 63.8183    & 91.3286    & 87.3871    & 2841.4514  \\ \hline
num_steps = 35, batch_size = 10               & 57.4925    & 87.9364    & 84.7437    & 2967.6752  \\ \hline
num_steps = 50, batch_size = 10               & 55.6267    & 87.5726    & 84.1104    & 3032.4830  \\ \hline
num_steps = 20, batch_size = 20               & 57.7876    & 89.4243    & 85.4032    & 5283.1615  \\ \hline
num_steps = 35, batch_size = 20               & 56.8357    & 89.0865    & 86.1965    & 5327.4472  \\ \hline
num_steps = 50, batch_size = 20               & 57.4257    & 89.8376    & 86.9127    & 5340.5170  \\ \hline
num_steps = 20, batch_size = 40               & 58.0128    & 91.2085    & 87.4406    & 7340.3399  \\ \hline
num_steps = 35, batch_size = 40               & 59.9359    & 92.4088    & 89.0932    & 7222.8050  \\ \hline
num_steps = 50, batch_size = 40               & 63.9817    & 95.3053    & 91.6211    & 7685.0879  \\ \hline
loss_function = full_softmax                  & 56.7543    & 88.6126    & 85.5887    & 5318.9263  \\ \hline
loss_function = sampled_softmax               & 4.7374     & 99.3000    & 94.9911    & 6250.8133  \\ \hline
max_grad_norm = 1.0                           & 103.9493   & 120.8001   & 118.1674   & 5322.4864  \\ \hline
max_grad_norm = 5.0                           & 56.8391    & 89.2325    & 85.9046    & 5309.6957  \\ \hline
max_grad_norm = 10.0                          & 61.9130    & 90.0071    & 86.5227    & 5328.3798  \\ \hline
optimizer = GradDesc                          & 56.5491    & 89.0828    & 85.6273    & 5324.8581  \\ \hline
optimizer = Adadelta                          & 764.3933   & 720.4745   & 689.8613   & 5188.6086  \\ \hline
optimizer = Adagrad                           & 67.8530    & 97.5119    & 93.7309    & 5261.3310  \\ \hline
optimizer = Adam                              & 66.7381    & 118.1673   & 108.4468   & 4806.0537  \\ \hline
optimizer = Momentum                          & 58.1349    & 88.0675    & 84.2937    & 5264.8214  \\ \hline
