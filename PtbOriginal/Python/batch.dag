JOB num_steps-batch_size0 batch.job DIR . 
JOB num_steps-batch_size1 batch.job DIR . 
JOB num_steps-batch_size2 batch.job DIR . 
JOB num_steps-batch_size3 batch.job DIR . 
JOB num_steps-batch_size4 batch.job DIR . 
JOB num_steps-batch_size5 batch.job DIR . 
JOB num_steps-batch_size6 batch.job DIR . 
JOB num_steps-batch_size7 batch.job DIR . 
JOB num_steps-batch_size8 batch.job DIR . 

VARS num_steps-batch_size0 vocab_size="10000" optimizer="GradDesc" keep_prob="0.5" max_grad_norm="5" num_layers="2" batch_size="10" init_scale="0.05" max_epoch="6" num_steps="20" max_max_epoch="39" lr_decay="0.8" loss_function="sequence_loss_by_example" hidden_size="512" learning_rate="1" embedded_size="256" num_run="0" test_name="num_steps-batch_size" 
VARS num_steps-batch_size1 vocab_size="10000" optimizer="GradDesc" keep_prob="0.5" max_grad_norm="5" num_layers="2" batch_size="10" init_scale="0.05" max_epoch="6" num_steps="35" max_max_epoch="39" lr_decay="0.8" loss_function="sequence_loss_by_example" hidden_size="512" learning_rate="1" embedded_size="256" num_run="1" test_name="num_steps-batch_size" 
VARS num_steps-batch_size2 vocab_size="10000" optimizer="GradDesc" keep_prob="0.5" max_grad_norm="5" num_layers="2" batch_size="10" init_scale="0.05" max_epoch="6" num_steps="50" max_max_epoch="39" lr_decay="0.8" loss_function="sequence_loss_by_example" hidden_size="512" learning_rate="1" embedded_size="256" num_run="2" test_name="num_steps-batch_size" 
VARS num_steps-batch_size3 vocab_size="10000" optimizer="GradDesc" keep_prob="0.5" max_grad_norm="5" num_layers="2" batch_size="20" init_scale="0.05" max_epoch="6" num_steps="20" max_max_epoch="39" lr_decay="0.8" loss_function="sequence_loss_by_example" hidden_size="512" learning_rate="1" embedded_size="256" num_run="3" test_name="num_steps-batch_size" 
VARS num_steps-batch_size4 vocab_size="10000" optimizer="GradDesc" keep_prob="0.5" max_grad_norm="5" num_layers="2" batch_size="20" init_scale="0.05" max_epoch="6" num_steps="35" max_max_epoch="39" lr_decay="0.8" loss_function="sequence_loss_by_example" hidden_size="512" learning_rate="1" embedded_size="256" num_run="4" test_name="num_steps-batch_size" 
VARS num_steps-batch_size5 vocab_size="10000" optimizer="GradDesc" keep_prob="0.5" max_grad_norm="5" num_layers="2" batch_size="20" init_scale="0.05" max_epoch="6" num_steps="50" max_max_epoch="39" lr_decay="0.8" loss_function="sequence_loss_by_example" hidden_size="512" learning_rate="1" embedded_size="256" num_run="5" test_name="num_steps-batch_size" 
VARS num_steps-batch_size6 vocab_size="10000" optimizer="GradDesc" keep_prob="0.5" max_grad_norm="5" num_layers="2" batch_size="40" init_scale="0.05" max_epoch="6" num_steps="20" max_max_epoch="39" lr_decay="0.8" loss_function="sequence_loss_by_example" hidden_size="512" learning_rate="1" embedded_size="256" num_run="6" test_name="num_steps-batch_size" 
VARS num_steps-batch_size7 vocab_size="10000" optimizer="GradDesc" keep_prob="0.5" max_grad_norm="5" num_layers="2" batch_size="40" init_scale="0.05" max_epoch="6" num_steps="35" max_max_epoch="39" lr_decay="0.8" loss_function="sequence_loss_by_example" hidden_size="512" learning_rate="1" embedded_size="256" num_run="7" test_name="num_steps-batch_size" 
VARS num_steps-batch_size8 vocab_size="10000" optimizer="GradDesc" keep_prob="0.5" max_grad_norm="5" num_layers="2" batch_size="40" init_scale="0.05" max_epoch="6" num_steps="50" max_max_epoch="39" lr_decay="0.8" loss_function="sequence_loss_by_example" hidden_size="512" learning_rate="1" embedded_size="256" num_run="8" test_name="num_steps-batch_size" 
