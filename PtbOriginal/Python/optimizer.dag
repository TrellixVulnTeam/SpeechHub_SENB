JOB optimizer0 optimizer.job DIR . 
JOB optimizer1 optimizer.job DIR . 
JOB optimizer2 optimizer.job DIR . 
JOB optimizer3 optimizer.job DIR . 
JOB optimizer4 optimizer.job DIR . 

VARS optimizer0 vocab_size="10000" optimizer="GradDesc" keep_prob="0.5" max_grad_norm="5" num_layers="2" batch_size="20" init_scale="0.05" max_epoch="6" num_steps="35" max_max_epoch="39" lr_decay="0.8" loss_function="sequence_loss_by_example" hidden_size="512" learning_rate="1" embedded_size="256" num_run="0" test_name="optimizer" 
VARS optimizer1 vocab_size="10000" optimizer="Adadelta" keep_prob="0.5" max_grad_norm="5" num_layers="2" batch_size="20" init_scale="0.05" max_epoch="6" num_steps="35" max_max_epoch="39" lr_decay="0.8" loss_function="sequence_loss_by_example" hidden_size="512" learning_rate="1" embedded_size="256" num_run="1" test_name="optimizer" 
VARS optimizer2 vocab_size="10000" optimizer="Adagrad" keep_prob="0.5" max_grad_norm="5" num_layers="2" batch_size="20" init_scale="0.05" max_epoch="6" num_steps="35" max_max_epoch="39" lr_decay="0.8" loss_function="sequence_loss_by_example" hidden_size="512" learning_rate="1" embedded_size="256" num_run="2" test_name="optimizer" 
VARS optimizer3 vocab_size="10000" optimizer="Momentum" keep_prob="0.5" max_grad_norm="5" num_layers="2" batch_size="20" init_scale="0.05" max_epoch="6" num_steps="35" max_max_epoch="39" lr_decay="0.8" loss_function="sequence_loss_by_example" hidden_size="512" learning_rate="1" embedded_size="256" num_run="3" test_name="optimizer" 
VARS optimizer4 vocab_size="10000" optimizer="Adam" keep_prob="0.5" max_grad_norm="5" num_layers="2" batch_size="20" init_scale="0.05" max_epoch="6" num_steps="35" max_max_epoch="39" lr_decay="0.8" loss_function="sequence_loss_by_example" hidden_size="512" learning_rate="1" embedded_size="256" num_run="4" test_name="optimizer" 
