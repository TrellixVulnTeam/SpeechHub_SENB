05/19/17 10:10:35 ******************************************************
05/19/17 10:10:35 ** condor_scheduniv_exec.211.0 (CONDOR_DAGMAN) STARTING UP
05/19/17 10:10:35 ** /usr/bin/condor_dagman
05/19/17 10:10:35 ** SubsystemInfo: name=DAGMAN type=DAGMAN(10) class=DAEMON(1)
05/19/17 10:10:35 ** Configuration: subsystem:DAGMAN local:<NONE> class:DAEMON
05/19/17 10:10:35 ** $CondorVersion: 8.6.2 Apr 23 2017 BuildID: 404257 $
05/19/17 10:10:35 ** $CondorPlatform: x86_64_RedHat7 $
05/19/17 10:10:35 ** PID = 29091
05/19/17 10:10:35 ** Log last touched time unavailable (No such file or directory)
05/19/17 10:10:35 ******************************************************
05/19/17 10:10:35 Using config source: /etc/condor/condor_config
05/19/17 10:10:35 Using local config sources: 
05/19/17 10:10:35    /etc/condor/config.d/00esat.config
05/19/17 10:10:35    /etc/condor/config.d/01java.config
05/19/17 10:10:35    /etc/condor/config.d/80optout_users.config
05/19/17 10:10:35    /etc/condor/condor_config.local
05/19/17 10:10:35 config Macros = 174, Sorted = 174, StringBytes = 6649, TablesBytes = 6336
05/19/17 10:10:35 CLASSAD_CACHING is ENABLED
05/19/17 10:10:35 Daemon Log is logging: D_ALWAYS D_ERROR
05/19/17 10:10:35 DaemonCore: No command port requested.
05/19/17 10:10:35 DAGMAN_USE_STRICT setting: 1
05/19/17 10:10:35 DAGMAN_VERBOSITY setting: 3
05/19/17 10:10:35 DAGMAN_DEBUG_CACHE_SIZE setting: 5242880
05/19/17 10:10:35 DAGMAN_DEBUG_CACHE_ENABLE setting: False
05/19/17 10:10:35 DAGMAN_SUBMIT_DELAY setting: 0
05/19/17 10:10:35 DAGMAN_MAX_SUBMIT_ATTEMPTS setting: 6
05/19/17 10:10:35 DAGMAN_STARTUP_CYCLE_DETECT setting: False
05/19/17 10:10:35 DAGMAN_MAX_SUBMITS_PER_INTERVAL setting: 5
05/19/17 10:10:35 DAGMAN_USER_LOG_SCAN_INTERVAL setting: 5
05/19/17 10:10:35 DAGMAN_DEFAULT_PRIORITY setting: 0
05/19/17 10:10:35 DAGMAN_SUPPRESS_NOTIFICATION setting: True
05/19/17 10:10:35 allow_events (DAGMAN_ALLOW_EVENTS) setting: 114
05/19/17 10:10:35 DAGMAN_RETRY_SUBMIT_FIRST setting: True
05/19/17 10:10:35 DAGMAN_RETRY_NODE_FIRST setting: False
05/19/17 10:10:35 DAGMAN_MAX_JOBS_IDLE setting: 1000
05/19/17 10:10:35 DAGMAN_MAX_JOBS_SUBMITTED setting: 0
05/19/17 10:10:35 DAGMAN_MAX_PRE_SCRIPTS setting: 20
05/19/17 10:10:35 DAGMAN_MAX_POST_SCRIPTS setting: 20
05/19/17 10:10:35 DAGMAN_MUNGE_NODE_NAMES setting: True
05/19/17 10:10:35 DAGMAN_PROHIBIT_MULTI_JOBS setting: False
05/19/17 10:10:35 DAGMAN_SUBMIT_DEPTH_FIRST setting: False
05/19/17 10:10:35 DAGMAN_ALWAYS_RUN_POST setting: False
05/19/17 10:10:35 DAGMAN_ABORT_DUPLICATES setting: True
05/19/17 10:10:35 DAGMAN_ABORT_ON_SCARY_SUBMIT setting: True
05/19/17 10:10:35 DAGMAN_PENDING_REPORT_INTERVAL setting: 600
05/19/17 10:10:35 DAGMAN_AUTO_RESCUE setting: True
05/19/17 10:10:35 DAGMAN_MAX_RESCUE_NUM setting: 100
05/19/17 10:10:35 DAGMAN_WRITE_PARTIAL_RESCUE setting: True
05/19/17 10:10:35 DAGMAN_DEFAULT_NODE_LOG setting: @(DAG_DIR)/@(DAG_FILE).nodes.log
05/19/17 10:10:35 DAGMAN_GENERATE_SUBDAG_SUBMITS setting: True
05/19/17 10:10:35 DAGMAN_MAX_JOB_HOLDS setting: 100
05/19/17 10:10:35 DAGMAN_HOLD_CLAIM_TIME setting: 20
05/19/17 10:10:35 ALL_DEBUG setting: 
05/19/17 10:10:35 DAGMAN_DEBUG setting: 
05/19/17 10:10:35 DAGMAN_SUPPRESS_JOB_LOGS setting: False
05/19/17 10:10:35 DAGMAN_REMOVE_NODE_JOBS setting: True
05/19/17 10:10:35 argv[0] == "condor_scheduniv_exec.211.0"
05/19/17 10:10:35 argv[1] == "-Lockfile"
05/19/17 10:10:35 argv[2] == "tied_models_large.dag.lock"
05/19/17 10:10:35 argv[3] == "-AutoRescue"
05/19/17 10:10:35 argv[4] == "1"
05/19/17 10:10:35 argv[5] == "-DoRescueFrom"
05/19/17 10:10:35 argv[6] == "0"
05/19/17 10:10:35 argv[7] == "-Dag"
05/19/17 10:10:35 argv[8] == "tied_models_large.dag"
05/19/17 10:10:35 argv[9] == "-Suppress_notification"
05/19/17 10:10:35 argv[10] == "-CsdVersion"
05/19/17 10:10:35 argv[11] == "$CondorVersion: 8.6.2 Apr 23 2017 BuildID: 404257 $"
05/19/17 10:10:35 argv[12] == "-Dagman"
05/19/17 10:10:35 argv[13] == "/usr/bin/condor_dagman"
05/19/17 10:10:35 Workflow batch-name: <tied_models_large.dag+211>
05/19/17 10:10:35 Workflow accounting_group: <>
05/19/17 10:10:35 Workflow accounting_group_user: <>
05/19/17 10:10:35 Warning: failed to get attribute DAGNodeName
05/19/17 10:10:35 DAGMAN_LOG_ON_NFS_IS_ERROR setting: False
05/19/17 10:10:35 Default node log file is: </esat/spchtemp/scratch/robbe/SpeechHub/n-best/python/jobdag/./tied_models_large.dag.nodes.log>
05/19/17 10:10:35 DAG Lockfile will be written to tied_models_large.dag.lock
05/19/17 10:10:35 DAG Input file is tied_models_large.dag
05/19/17 10:10:35 Parsing 1 dagfiles
05/19/17 10:10:35 Parsing tied_models_large.dag ...
05/19/17 10:10:35 Dag contains 2 total jobs
05/19/17 10:10:35 Sleeping for 3 seconds to ensure ProcessId uniqueness
05/19/17 10:10:38 Bootstrapping...
05/19/17 10:10:38 Number of pre-completed nodes: 0
05/19/17 10:10:38 MultiLogFiles: truncating log file /esat/spchtemp/scratch/robbe/SpeechHub/n-best/python/jobdag/./tied_models_large.dag.nodes.log
05/19/17 10:10:38 DAG status: 0 (DAG_STATUS_OK)
05/19/17 10:10:38 Of 2 nodes total:
05/19/17 10:10:38  Done     Pre   Queued    Post   Ready   Un-Ready   Failed
05/19/17 10:10:38   ===     ===      ===     ===     ===        ===      ===
05/19/17 10:10:38     0       0        0       0       2          0        0
05/19/17 10:10:38 0 job proc(s) currently held
05/19/17 10:10:38 Registering condor_event_timer...
05/19/17 10:10:39 Submitting HTCondor Node topic job(s)...
05/19/17 10:10:39 Adding a DAGMan workflow log /esat/spchtemp/scratch/robbe/SpeechHub/n-best/python/jobdag/./tied_models_large.dag.nodes.log
05/19/17 10:10:39 Masking the events recorded in the DAGMAN workflow log
05/19/17 10:10:39 Mask for workflow log is 0,1,2,4,5,7,9,10,11,12,13,16,17,24,27
05/19/17 10:10:39 submitting: /usr/bin/condor_submit -a dag_node_name' '=' 'topic -a +DAGManJobId' '=' '211 -a DAGManJobId' '=' '211 -batch-name tied_models_large.dag+211 -a submit_event_notes' '=' 'DAG' 'Node:' 'topic -a dagman_log' '=' '/esat/spchtemp/scratch/robbe/SpeechHub/n-best/python/jobdag/./tied_models_large.dag.nodes.log -a +DAGManNodesMask' '=' '"0,1,2,4,5,7,9,10,11,12,13,16,17,24,27" -a test_name' '=' 'topic -a num_run' '=' '0 -a model_name' '=' 'tied_models_large -a name' '=' 'tied_n_best_topic_large -a DAG_STATUS' '=' '0 -a FAILED_COUNT' '=' '0 -a notification' '=' 'never -a +DAGParentNodeNames' '=' '"" jobdag/topic_tied.job
05/19/17 10:10:39 From submit: Submitting job(s)
05/19/17 10:10:39 From submit: ERROR: on Line 14 of submit file: 
05/19/17 10:10:39 From submit: 
05/19/17 10:10:39 From submit: ERROR: Failed to parse command file (line 14).
05/19/17 10:10:39 failed while reading from pipe.
05/19/17 10:10:39 Read so far: Submitting job(s)ERROR: on Line 14 of submit file: ERROR: Failed to parse command file (line 14).
05/19/17 10:10:39 ERROR: submit attempt failed
05/19/17 10:10:39 submit command was: /usr/bin/condor_submit -a dag_node_name' '=' 'topic -a +DAGManJobId' '=' '211 -a DAGManJobId' '=' '211 -batch-name tied_models_large.dag+211 -a submit_event_notes' '=' 'DAG' 'Node:' 'topic -a dagman_log' '=' '/esat/spchtemp/scratch/robbe/SpeechHub/n-best/python/jobdag/./tied_models_large.dag.nodes.log -a +DAGManNodesMask' '=' '"0,1,2,4,5,7,9,10,11,12,13,16,17,24,27" -a test_name' '=' 'topic -a num_run' '=' '0 -a model_name' '=' 'tied_models_large -a name' '=' 'tied_n_best_topic_large -a DAG_STATUS' '=' '0 -a FAILED_COUNT' '=' '0 -a notification' '=' 'never -a +DAGParentNodeNames' '=' '"" jobdag/topic_tied.job
05/19/17 10:10:39 Job submit try 1/6 failed, will try again in >= 1 second.
05/19/17 10:10:39 DAG status: 0 (DAG_STATUS_OK)
05/19/17 10:10:39 Of 2 nodes total:
05/19/17 10:10:39  Done     Pre   Queued    Post   Ready   Un-Ready   Failed
05/19/17 10:10:39   ===     ===      ===     ===     ===        ===      ===
05/19/17 10:10:39     0       0        0       0       2          0        0
05/19/17 10:10:39 0 job proc(s) currently held
05/19/17 10:10:44 Submitting HTCondor Node topic job(s)...
05/19/17 10:10:44 Adding a DAGMan workflow log /esat/spchtemp/scratch/robbe/SpeechHub/n-best/python/jobdag/./tied_models_large.dag.nodes.log
05/19/17 10:10:44 Masking the events recorded in the DAGMAN workflow log
05/19/17 10:10:44 Mask for workflow log is 0,1,2,4,5,7,9,10,11,12,13,16,17,24,27
05/19/17 10:10:44 submitting: /usr/bin/condor_submit -a dag_node_name' '=' 'topic -a +DAGManJobId' '=' '211 -a DAGManJobId' '=' '211 -batch-name tied_models_large.dag+211 -a submit_event_notes' '=' 'DAG' 'Node:' 'topic -a dagman_log' '=' '/esat/spchtemp/scratch/robbe/SpeechHub/n-best/python/jobdag/./tied_models_large.dag.nodes.log -a +DAGManNodesMask' '=' '"0,1,2,4,5,7,9,10,11,12,13,16,17,24,27" -a test_name' '=' 'topic -a num_run' '=' '0 -a model_name' '=' 'tied_models_large -a name' '=' 'tied_n_best_topic_large -a DAG_STATUS' '=' '0 -a FAILED_COUNT' '=' '0 -a notification' '=' 'never -a +DAGParentNodeNames' '=' '"" jobdag/topic_tied.job
05/19/17 10:10:44 From submit: Submitting job(s)
05/19/17 10:10:44 From submit: ERROR: on Line 14 of submit file: 
05/19/17 10:10:44 From submit: 
05/19/17 10:10:44 From submit: ERROR: Failed to parse command file (line 14).
05/19/17 10:10:44 failed while reading from pipe.
05/19/17 10:10:44 Read so far: Submitting job(s)ERROR: on Line 14 of submit file: ERROR: Failed to parse command file (line 14).
05/19/17 10:10:44 ERROR: submit attempt failed
05/19/17 10:10:44 submit command was: /usr/bin/condor_submit -a dag_node_name' '=' 'topic -a +DAGManJobId' '=' '211 -a DAGManJobId' '=' '211 -batch-name tied_models_large.dag+211 -a submit_event_notes' '=' 'DAG' 'Node:' 'topic -a dagman_log' '=' '/esat/spchtemp/scratch/robbe/SpeechHub/n-best/python/jobdag/./tied_models_large.dag.nodes.log -a +DAGManNodesMask' '=' '"0,1,2,4,5,7,9,10,11,12,13,16,17,24,27" -a test_name' '=' 'topic -a num_run' '=' '0 -a model_name' '=' 'tied_models_large -a name' '=' 'tied_n_best_topic_large -a DAG_STATUS' '=' '0 -a FAILED_COUNT' '=' '0 -a notification' '=' 'never -a +DAGParentNodeNames' '=' '"" jobdag/topic_tied.job
05/19/17 10:10:44 Job submit try 2/6 failed, will try again in >= 2 seconds.
05/19/17 10:10:49 Submitting HTCondor Node topic job(s)...
05/19/17 10:10:49 Adding a DAGMan workflow log /esat/spchtemp/scratch/robbe/SpeechHub/n-best/python/jobdag/./tied_models_large.dag.nodes.log
05/19/17 10:10:49 Masking the events recorded in the DAGMAN workflow log
05/19/17 10:10:49 Mask for workflow log is 0,1,2,4,5,7,9,10,11,12,13,16,17,24,27
05/19/17 10:10:49 submitting: /usr/bin/condor_submit -a dag_node_name' '=' 'topic -a +DAGManJobId' '=' '211 -a DAGManJobId' '=' '211 -batch-name tied_models_large.dag+211 -a submit_event_notes' '=' 'DAG' 'Node:' 'topic -a dagman_log' '=' '/esat/spchtemp/scratch/robbe/SpeechHub/n-best/python/jobdag/./tied_models_large.dag.nodes.log -a +DAGManNodesMask' '=' '"0,1,2,4,5,7,9,10,11,12,13,16,17,24,27" -a test_name' '=' 'topic -a num_run' '=' '0 -a model_name' '=' 'tied_models_large -a name' '=' 'tied_n_best_topic_large -a DAG_STATUS' '=' '0 -a FAILED_COUNT' '=' '0 -a notification' '=' 'never -a +DAGParentNodeNames' '=' '"" jobdag/topic_tied.job
05/19/17 10:10:49 From submit: Submitting job(s)
05/19/17 10:10:49 From submit: ERROR: on Line 14 of submit file: 
05/19/17 10:10:49 From submit: 
05/19/17 10:10:49 From submit: ERROR: Failed to parse command file (line 14).
05/19/17 10:10:49 failed while reading from pipe.
05/19/17 10:10:49 Read so far: Submitting job(s)ERROR: on Line 14 of submit file: ERROR: Failed to parse command file (line 14).
05/19/17 10:10:49 ERROR: submit attempt failed
05/19/17 10:10:49 submit command was: /usr/bin/condor_submit -a dag_node_name' '=' 'topic -a +DAGManJobId' '=' '211 -a DAGManJobId' '=' '211 -batch-name tied_models_large.dag+211 -a submit_event_notes' '=' 'DAG' 'Node:' 'topic -a dagman_log' '=' '/esat/spchtemp/scratch/robbe/SpeechHub/n-best/python/jobdag/./tied_models_large.dag.nodes.log -a +DAGManNodesMask' '=' '"0,1,2,4,5,7,9,10,11,12,13,16,17,24,27" -a test_name' '=' 'topic -a num_run' '=' '0 -a model_name' '=' 'tied_models_large -a name' '=' 'tied_n_best_topic_large -a DAG_STATUS' '=' '0 -a FAILED_COUNT' '=' '0 -a notification' '=' 'never -a +DAGParentNodeNames' '=' '"" jobdag/topic_tied.job
05/19/17 10:10:49 Job submit try 3/6 failed, will try again in >= 4 seconds.
05/19/17 10:10:54 Submitting HTCondor Node topic job(s)...
05/19/17 10:10:54 Adding a DAGMan workflow log /esat/spchtemp/scratch/robbe/SpeechHub/n-best/python/jobdag/./tied_models_large.dag.nodes.log
05/19/17 10:10:54 Masking the events recorded in the DAGMAN workflow log
05/19/17 10:10:54 Mask for workflow log is 0,1,2,4,5,7,9,10,11,12,13,16,17,24,27
05/19/17 10:10:54 submitting: /usr/bin/condor_submit -a dag_node_name' '=' 'topic -a +DAGManJobId' '=' '211 -a DAGManJobId' '=' '211 -batch-name tied_models_large.dag+211 -a submit_event_notes' '=' 'DAG' 'Node:' 'topic -a dagman_log' '=' '/esat/spchtemp/scratch/robbe/SpeechHub/n-best/python/jobdag/./tied_models_large.dag.nodes.log -a +DAGManNodesMask' '=' '"0,1,2,4,5,7,9,10,11,12,13,16,17,24,27" -a test_name' '=' 'topic -a num_run' '=' '0 -a model_name' '=' 'tied_models_large -a name' '=' 'tied_n_best_topic_large -a DAG_STATUS' '=' '0 -a FAILED_COUNT' '=' '0 -a notification' '=' 'never -a +DAGParentNodeNames' '=' '"" jobdag/topic_tied.job
05/19/17 10:10:54 From submit: Submitting job(s)
05/19/17 10:10:54 From submit: ERROR: on Line 14 of submit file: 
05/19/17 10:10:54 From submit: 
05/19/17 10:10:54 From submit: ERROR: Failed to parse command file (line 14).
05/19/17 10:10:54 failed while reading from pipe.
05/19/17 10:10:54 Read so far: Submitting job(s)ERROR: on Line 14 of submit file: ERROR: Failed to parse command file (line 14).
05/19/17 10:10:54 ERROR: submit attempt failed
05/19/17 10:10:54 submit command was: /usr/bin/condor_submit -a dag_node_name' '=' 'topic -a +DAGManJobId' '=' '211 -a DAGManJobId' '=' '211 -batch-name tied_models_large.dag+211 -a submit_event_notes' '=' 'DAG' 'Node:' 'topic -a dagman_log' '=' '/esat/spchtemp/scratch/robbe/SpeechHub/n-best/python/jobdag/./tied_models_large.dag.nodes.log -a +DAGManNodesMask' '=' '"0,1,2,4,5,7,9,10,11,12,13,16,17,24,27" -a test_name' '=' 'topic -a num_run' '=' '0 -a model_name' '=' 'tied_models_large -a name' '=' 'tied_n_best_topic_large -a DAG_STATUS' '=' '0 -a FAILED_COUNT' '=' '0 -a notification' '=' 'never -a +DAGParentNodeNames' '=' '"" jobdag/topic_tied.job
05/19/17 10:10:54 Job submit try 4/6 failed, will try again in >= 8 seconds.
05/19/17 10:11:05 Submitting HTCondor Node topic job(s)...
05/19/17 10:11:05 Adding a DAGMan workflow log /esat/spchtemp/scratch/robbe/SpeechHub/n-best/python/jobdag/./tied_models_large.dag.nodes.log
05/19/17 10:11:05 Masking the events recorded in the DAGMAN workflow log
05/19/17 10:11:05 Mask for workflow log is 0,1,2,4,5,7,9,10,11,12,13,16,17,24,27
05/19/17 10:11:05 submitting: /usr/bin/condor_submit -a dag_node_name' '=' 'topic -a +DAGManJobId' '=' '211 -a DAGManJobId' '=' '211 -batch-name tied_models_large.dag+211 -a submit_event_notes' '=' 'DAG' 'Node:' 'topic -a dagman_log' '=' '/esat/spchtemp/scratch/robbe/SpeechHub/n-best/python/jobdag/./tied_models_large.dag.nodes.log -a +DAGManNodesMask' '=' '"0,1,2,4,5,7,9,10,11,12,13,16,17,24,27" -a test_name' '=' 'topic -a num_run' '=' '0 -a model_name' '=' 'tied_models_large -a name' '=' 'tied_n_best_topic_large -a DAG_STATUS' '=' '0 -a FAILED_COUNT' '=' '0 -a notification' '=' 'never -a +DAGParentNodeNames' '=' '"" jobdag/topic_tied.job
05/19/17 10:11:05 From submit: Submitting job(s)
05/19/17 10:11:05 From submit: ERROR: on Line 14 of submit file: 
05/19/17 10:11:05 From submit: 
05/19/17 10:11:05 From submit: ERROR: Failed to parse command file (line 14).
05/19/17 10:11:05 failed while reading from pipe.
05/19/17 10:11:05 Read so far: Submitting job(s)ERROR: on Line 14 of submit file: ERROR: Failed to parse command file (line 14).
05/19/17 10:11:05 ERROR: submit attempt failed
05/19/17 10:11:05 submit command was: /usr/bin/condor_submit -a dag_node_name' '=' 'topic -a +DAGManJobId' '=' '211 -a DAGManJobId' '=' '211 -batch-name tied_models_large.dag+211 -a submit_event_notes' '=' 'DAG' 'Node:' 'topic -a dagman_log' '=' '/esat/spchtemp/scratch/robbe/SpeechHub/n-best/python/jobdag/./tied_models_large.dag.nodes.log -a +DAGManNodesMask' '=' '"0,1,2,4,5,7,9,10,11,12,13,16,17,24,27" -a test_name' '=' 'topic -a num_run' '=' '0 -a model_name' '=' 'tied_models_large -a name' '=' 'tied_n_best_topic_large -a DAG_STATUS' '=' '0 -a FAILED_COUNT' '=' '0 -a notification' '=' 'never -a +DAGParentNodeNames' '=' '"" jobdag/topic_tied.job
05/19/17 10:11:05 Job submit try 5/6 failed, will try again in >= 16 seconds.
05/19/17 10:11:22 Submitting HTCondor Node topic job(s)...
05/19/17 10:11:22 Adding a DAGMan workflow log /esat/spchtemp/scratch/robbe/SpeechHub/n-best/python/jobdag/./tied_models_large.dag.nodes.log
05/19/17 10:11:22 Masking the events recorded in the DAGMAN workflow log
05/19/17 10:11:22 Mask for workflow log is 0,1,2,4,5,7,9,10,11,12,13,16,17,24,27
05/19/17 10:11:22 submitting: /usr/bin/condor_submit -a dag_node_name' '=' 'topic -a +DAGManJobId' '=' '211 -a DAGManJobId' '=' '211 -batch-name tied_models_large.dag+211 -a submit_event_notes' '=' 'DAG' 'Node:' 'topic -a dagman_log' '=' '/esat/spchtemp/scratch/robbe/SpeechHub/n-best/python/jobdag/./tied_models_large.dag.nodes.log -a +DAGManNodesMask' '=' '"0,1,2,4,5,7,9,10,11,12,13,16,17,24,27" -a test_name' '=' 'topic -a num_run' '=' '0 -a model_name' '=' 'tied_models_large -a name' '=' 'tied_n_best_topic_large -a DAG_STATUS' '=' '0 -a FAILED_COUNT' '=' '0 -a notification' '=' 'never -a +DAGParentNodeNames' '=' '"" jobdag/topic_tied.job
05/19/17 10:11:22 From submit: Submitting job(s)
05/19/17 10:11:22 From submit: ERROR: on Line 14 of submit file: 
05/19/17 10:11:22 From submit: 
05/19/17 10:11:22 From submit: ERROR: Failed to parse command file (line 14).
05/19/17 10:11:22 failed while reading from pipe.
05/19/17 10:11:22 Read so far: Submitting job(s)ERROR: on Line 14 of submit file: ERROR: Failed to parse command file (line 14).
05/19/17 10:11:22 ERROR: submit attempt failed
05/19/17 10:11:22 submit command was: /usr/bin/condor_submit -a dag_node_name' '=' 'topic -a +DAGManJobId' '=' '211 -a DAGManJobId' '=' '211 -batch-name tied_models_large.dag+211 -a submit_event_notes' '=' 'DAG' 'Node:' 'topic -a dagman_log' '=' '/esat/spchtemp/scratch/robbe/SpeechHub/n-best/python/jobdag/./tied_models_large.dag.nodes.log -a +DAGManNodesMask' '=' '"0,1,2,4,5,7,9,10,11,12,13,16,17,24,27" -a test_name' '=' 'topic -a num_run' '=' '0 -a model_name' '=' 'tied_models_large -a name' '=' 'tied_n_best_topic_large -a DAG_STATUS' '=' '0 -a FAILED_COUNT' '=' '0 -a notification' '=' 'never -a +DAGParentNodeNames' '=' '"" jobdag/topic_tied.job
05/19/17 10:11:22 Job submit failed after 6 tries.
05/19/17 10:11:22 Shortcutting node topic retries because of submit failure(s)
05/19/17 10:11:22 DAG status: 2 (DAG_STATUS_NODE_FAILED)
05/19/17 10:11:22 Of 2 nodes total:
05/19/17 10:11:22  Done     Pre   Queued    Post   Ready   Un-Ready   Failed
05/19/17 10:11:22   ===     ===      ===     ===     ===        ===      ===
05/19/17 10:11:22     0       0        0       0       1          0        1
05/19/17 10:11:22 0 job proc(s) currently held
05/19/17 10:11:27 Submitting HTCondor Node extended_topic_1 job(s)...
05/19/17 10:11:27 Adding a DAGMan workflow log /esat/spchtemp/scratch/robbe/SpeechHub/n-best/python/jobdag/./tied_models_large.dag.nodes.log
05/19/17 10:11:27 Masking the events recorded in the DAGMAN workflow log
05/19/17 10:11:27 Mask for workflow log is 0,1,2,4,5,7,9,10,11,12,13,16,17,24,27
05/19/17 10:11:27 submitting: /usr/bin/condor_submit -a dag_node_name' '=' 'extended_topic_1 -a +DAGManJobId' '=' '211 -a DAGManJobId' '=' '211 -batch-name tied_models_large.dag+211 -a submit_event_notes' '=' 'DAG' 'Node:' 'extended_topic_1 -a dagman_log' '=' '/esat/spchtemp/scratch/robbe/SpeechHub/n-best/python/jobdag/./tied_models_large.dag.nodes.log -a +DAGManNodesMask' '=' '"0,1,2,4,5,7,9,10,11,12,13,16,17,24,27" -a test_name' '=' 'extended_topic_1 -a num_run' '=' '0 -a model_name' '=' 'tied_models_large -a name' '=' 'tied_n_best_textended_topic_1_large -a DAG_STATUS' '=' '2 -a FAILED_COUNT' '=' '1 -a notification' '=' 'never -a +DAGParentNodeNames' '=' '"" jobdag/extended_topic_1_tied.job
05/19/17 10:11:28 From submit: Submitting job(s).
05/19/17 10:11:28 From submit: 1 job(s) submitted to cluster 212.
05/19/17 10:11:28 	assigned HTCondor ID (212.0.0)
05/19/17 10:11:28 Just submitted 1 job this cycle...
05/19/17 10:11:28 Currently monitoring 1 HTCondor log file(s)
05/19/17 10:11:28 Reassigning the id of job extended_topic_1 from (212.0.0) to (212.0.0)
05/19/17 10:11:28 Event: ULOG_SUBMIT for HTCondor Node extended_topic_1 (212.0.0) {05/19/17 10:11:28}
05/19/17 10:11:28 Number of idle job procs: 1
05/19/17 10:11:28 DAG status: 2 (DAG_STATUS_NODE_FAILED)
05/19/17 10:11:28 Of 2 nodes total:
05/19/17 10:11:28  Done     Pre   Queued    Post   Ready   Un-Ready   Failed
05/19/17 10:11:28   ===     ===      ===     ===     ===        ===      ===
05/19/17 10:11:28     0       0        1       0       0          0        1
05/19/17 10:11:28 0 job proc(s) currently held
05/19/17 10:21:31 603 seconds since last log event
05/19/17 10:21:31 Pending DAG nodes:
05/19/17 10:21:31   Node extended_topic_1, HTCondor ID 212, status STATUS_SUBMITTED
05/19/17 10:31:34 1206 seconds since last log event
05/19/17 10:31:34 Pending DAG nodes:
05/19/17 10:31:34   Node extended_topic_1, HTCondor ID 212, status STATUS_SUBMITTED
05/19/17 10:41:37 1809 seconds since last log event
05/19/17 10:41:37 Pending DAG nodes:
05/19/17 10:41:37   Node extended_topic_1, HTCondor ID 212, status STATUS_SUBMITTED
05/19/17 10:51:40 2412 seconds since last log event
05/19/17 10:51:40 Pending DAG nodes:
05/19/17 10:51:40   Node extended_topic_1, HTCondor ID 212, status STATUS_SUBMITTED
